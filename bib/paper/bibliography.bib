@article{belady1966study,
  title={A study of replacement algorithms for a virtual-storage computer},
  author={Belady, Laszlo A.},
  journal={IBM Systems journal},
  volume={5},
  number={2},
  pages={78--101},
  year={1966},
  publisher={IBM}
}

@inproceedings{choi_20nm_2012,
	title = {A 20nm 1.8V 8Gb {PRAM} with 40MB/s program bandwidth},
	doi = {10.1109/ISSCC.2012.6176872},
	abstract = {Phase-change random access memory (PRAM) is considered as one of the most promising candidates for future memories because of its good scalability and cost-effectiveness [1]. Besides implementations with standard interfaces like NOR flash or LPDDR2-NVM, application-oriented approaches using PRAM as main-memory or storage-class memory have been researched [2-3]. These studies suggest that noticeable merits can be achieved by using PRAM in improving power consumption, system cost, etc. However, relatively low chip density and insufficient write bandwidth of PRAMs are obstacles to better system performance. In this paper, we present an 8Gb PRAM with 40MB/s write bandwidth featuring 8Mb sub-array core architecture with 20nm diode-switched PRAM cells [4]. When an external high voltage is applied, the write bandwidth can be extended as high as 133MB/s.},
	booktitle = {Solid-{State} {Circuits} {Conference} {Digest} of {Technical} {Papers}, 2012 {IEEE} {International}},
	author = {Choi, Youngdon and Song, Ickhyun and Park, Mu-Hui and Chung, Hoeju and Chang, Sanghoan and Cho, Beakhyoung and Kim, Jinyoung and Oh, Younghoon and Kwon, Duckmin and Sunwoo, Jung and Shin, Junho and Rho, Yoohwan and Lee, Changsoo and Kang, Min Gu and Lee, Jaeyun and Kwon, Yongjin and Kim, Soehee and Kim, Jaehwan and Lee, Yong-Jun and Wang, Qi and Cha, Sooho and Ahn, Sujin and Horii, H. and Lee, Jaewook and Kim, Kisung and Joo, Hansung and Lee, Kwangjin and Lee, Yeong-Taek and Yoo, Jeihwan and Jeong, G.},
	month = feb,
	year = {2012},
	keywords = {application-oriented approaches, Bandwidth, chip density, Computer architecture, diode-switched PRAM cells, flash memories, insufficient write bandwidth, low-power electronics, LPDDR2-NVM, main-memory, memory size 8 GByte, NOR circuits, NOR flash, phase change memories, Phase change random access memory, phase-change random access memory, power consumption, program bandwidth, Resistance, Sensors, size 20 nm, storage-class memory, sub-array core architecture, system cost, system performance, Threshold voltage, voltage 1.8 V, write-once storage},
	pages = {46--48},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/ES7BX7QD/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/SS4PDX3B/Choi 等. - 2012 - A 20nm 1.8V 8Gb PRAM with 40MBs program bandwidth.pdf:application/pdf}
}
@inproceedings{condit_better_2009,
	title = {Better {I}/{O} {Through} {Byte}-addressable, {Persistent} {Memory}},
	isbn = {978-1-60558-752-3},
	doi = {10.1145/1629575.1629589},
	abstract = {Modern computer systems have been built around the assumption that persistent storage is accessed via a slow, block-based interface. However, new byte-addressable, persistent memory technologies such as phase change memory (PCM) offer fast, fine-grained access to persistent storage. In this paper, we present a file system and a hardware architecture that are designed around the properties of persistent, byteaddressable memory. Our file system, BPFS, uses a new technique called short-circuit shadow paging to provide atomic, fine-grained updates to persistent storage. As a result, BPFS provides strong reliability guarantees and offers better performance than traditional file systems, even when both are run on top of byte-addressable, persistent memory. Our hardware architecture enforces atomicity and ordering guarantees required by BPFS while still providing the performance benefits of the L1 and L2 caches. Since these memory technologies are not yet widely available, we evaluate BPFS on DRAM against NTFS on both a RAM disk and a traditional disk. Then, we use microarchitectural simulations to estimate the performance of BPFS on PCM. Despite providing strong safety and consistency guarantees, BPFS on DRAM is typically twice as fast as NTFS on a RAM disk and 4-10 times faster than NTFS on disk. We also show that BPFS on PCM should be significantly faster than a traditional disk-based file system.},
	urldate = {2014-04-11},
	booktitle = {Proceedings of the {ACM} {SIGOPS} 22Nd {Symposium} on {Operating} {Systems} {Principles}},
	author = {Condit, Jeremy and Nightingale, Edmund B. and Frost, Christopher and Ipek, Engin and Lee, Benjamin and Burger, Doug and Coetzee, Derrick},
	year = {2009},
	keywords = {CCF-A, file systems, performance, phase change memory, Skim, Top},
	pages = {133--146},
	file = {ACM Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/7DK8KEFS/Condit 等. - 2009 - Better IO Through Byte-addressable, Persistent Me.pdf:application/pdf}
}
@inproceedings{dhiman_pdram_2009,
	title = {{PDRAM}: {A} hybrid {PRAM} and {DRAM} main memory system},
	shorttitle = {{PDRAM}},
	abstract = {In this paper, we propose PDRAM, a novel energy efficient main memory architecture based on phase change random access memory (PRAM) and DRAM. The paper explores the challenges involved in incorporating PRAM into the main memory hierarchy of computing systems, and proposes a low overhead hybrid hardware-software solution for managing it. Our experimental results indicate that our solution is able to achieve average energy savings of 30\% at negligible overhead over conventional memory architectures.},
	booktitle = {46th {ACM}/{IEEE} {Design} {Automation} {Conference}, 2009. {DAC} '09},
	author = {Dhiman, G. and Ayoub, R. and Rosing, T.},
	month = jul,
	year = {2009},
	keywords = {CCF-B, computing system, Costs, DRAM chips, DRAM main memory system, energy consumption, Energy efficiency, energy efficient PDRAM architecture, Energy management, Hardware, hybrid hardware-software solution, low-power electronics, memory architecture, Memory management, phase change memories, phase change memory, Phase change random access memory, Poster, Power system management, Random access memory, Skim, Top},
	pages = {664--669},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/SQVCCNRB/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/84SZMECB/Dhiman 等. - 2009 - PDRAM A hybrid PRAM and DRAM main memory system.pdf:application/pdf}
}

@inproceedings{dong_leveraging_2009,
	title = {Leveraging 3D {PCRAM} {Technologies} to {Reduce} {Checkpoint} {Overhead} for {Future} {Exascale} {Systems}},
	isbn = {978-1-60558-744-8},
	doi = {10.1145/1654059.1654117},
	abstract = {The scalability of future massively parallel processing (MPP) systems is challenged by high failure rates. Current hard disk drive (HDD) checkpointing results in overhead of 25\% or more at the petascale. With a direct correlation between checkpoint frequencies and node counts, novel techniques that can take more frequent checkpoints with minimum overhead are critical to implement a reliable exascale system. In this work, we leverage the upcoming Phase-Change Random Access Memory (PCRAM) technology and propose a hybrid local/global checkpointing mechanism after a thorough analysis of MPP systems failure rates and failure sources. We propose three variants of PCRAM-based hybrid checkpointing schemes, DIMM+HDD, DIMM+DIMM, and 3D+3D, to reduce the checkpoint overhead and offer a smooth transition from the conventional pure HDD checkpoint to the ideal 3D PCRAM mechanism. The proposed pure 3D PCRAM-based mechanism can ultimately take checkpoints with overhead less than 4\% on a projected exascale system.},
	urldate = {2014-04-10},
	booktitle = {Proceedings of the {Conference} on {High} {Performance} {Computing} {Networking}, {Storage} and {Analysis}},
	author = {Dong, Xiangyu and Muralimanohar, Naveen and Jouppi, Norm and Kaufmann, Richard and Xie, Yuan},
	year = {2009},
	keywords = {CCF-B, Skim, Top},
	pages = {57:1--57:12},
	file = {ACM Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/9NBNGKCQ/Dong 等. - 2009 - Leveraging 3D PCRAM Technologies to Reduce Checkpo.pdf:application/pdf}
}
@inproceedings{gao_real-time_2015,
	title = {Real-{Time} {In}-{Memory} {Checkpointing} for {Future} {Hybrid} {Memory} {Systems}},
	isbn = {978-1-4503-3559-1},
	doi = {10.1145/2751205.2751212},
	abstract = {In this paper, we study real-time in-memory checkpointing as an effective means to improve the reliability of future large-scale parallel processing systems. Under this context, the checkpoint overhead can become a significant performance bottleneck. Novel memory system designs with upcoming non-volatile random access memory (NVRAM) technologies are emerging to address this performance issue. However, we find that those designs can still have prohibitively high checkpoint overhead and system downtime, especially when checkpoints are taken frequently to implement a reliable system. In this paper, we propose a novel in-memory checkpointing system, named Mona, for reducing the checkpoint overhead of hybrid memory systems with NVRAM and DRAM. To minimize the in-memory checkpoint overhead, Mona dynamically writes partial checkpoints from DRAM to NVRAM during application execution. To reduce the interference of partial checkpointing, Mona utilizes runtime idle periods and leverages a cost model to guide partial checkpointing decisions for individual DRAM ranks. We further develop load-balancing mechanisms to balance checkpoint overheads across different DRAM ranks. Simulation results demonstrate the efficiency and effectiveness of Mona in reducing the checkpoint overhead, downtime and restarting time.},
	urldate = {2015-06-25},
	booktitle = {Proceedings of the 29th {ACM} on {International} {Conference} on {Supercomputing}},
	author = {Gao, Shen and He, Bingsheng and Xu, Jianliang},
	year = {2015},
	keywords = {CCF-B, checkpointing, nvram, parallel computing, phase change memory, Skim},
	pages = {263--272},
	file = {ACM Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/MF82VERB/Gao et al. - 2015 - Real-Time In-Memory Checkpointing for Future Hybri.pdf:application/pdf}
}
@inproceedings{ham_disintegrated_2013,
	title = {Disintegrated control for energy-efficient and heterogeneous memory systems},
	doi = {10.1109/HPCA.2013.6522338},
	abstract = {A mix of emerging technologies promises qualitatively new memory system capabilities. However, today's memory controllers and channels constrain heterogeneity. Today's integration of controllers on processor dies prevents systems from accommodating diverse, technology-specific protocols and schedulers in a modular manner; memory design decisions must be made during processor design. Moreover, today's channel architectures are not flexible enough to accommodate diverse demands for bandwidth and capacity. To address these challenges, we present strategies for scalability and heterogeneity, which include (i) disintegrating memory controllers to support heterogeneous command protocols in a modular manner; (ii) adding buffers to ensure signal integrity; and (iii) organizing buffers hierarchically to reduce latency. We apply these strategies to architect a novel heterogeneous DRAM / PCM system. Finally, we present mechanisms for power-efficient data movement.},
	booktitle = {2013 {IEEE} 19th {International} {Symposium} on {High} {Performance} {Computer} {Architecture}},
	author = {Ham, Tae Jun and Chelepalli, Bharath K. and Xue, Neng and Lee, Benjamin C.},
	month = feb,
	year = {2013},
	keywords = {CCF-A, memory controller, Skim, Top},
	pages = {424--435},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/IMMDI6JF/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/MG8RCD7X/Ham 等. - 2013 - Disintegrated control for energy-efficient and het.pdf:application/pdf}
}

@inproceedings{huang_improving_2005,
	title = {Improving energy efficiency by making {DRAM} less randomly accessed},
	doi = {10.1109/LPE.2005.195553},
	abstract = {Existing techniques manage power for the main memory by passively monitoring the memory traffic, and based on which, predict when to power down and into which low-power state to transition. However, passively monitoring the memory traffic can be far from being effective as idle periods between consecutive memory accesses are often too short for existing power-management techniques to take full advantage of the deeper power-saving state implemented in modem DRAM architectures. In this paper, the authors proposed a new technique that will actively reshape the memory traffic to coalesce short idle periods - which were previously unusable for power management - into longer ones, thus enabling existing techniques to effectively exploit idleness in the memory.},
	booktitle = {Proceedings of the 2005 {International} {Symposium} on {Low} {Power} {Electronics} and {Design}, 2005. {ISLPED} '05},
	author = {Huang, Hai and Shin, K.G. and Lefurgy, C. and Keller, T.},
	month = aug,
	year = {2005},
	keywords = {condition monitoring, DRAM, DRAM chips, Energy efficiency, energy efficiency improvement, Energy management, memory architecture, Memory management, memory traffic, Monitoring, passive monitoring, Permission, power management, Power system management, random access, Random access memory, Software measurement, Software performance, Software systems, storage management},
	pages = {393--398},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/3DDGHF4T/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/26JTREQJ/Huang et al. - 2005 - Improving energy efficiency by making DRAM less ra.pdf:application/pdf}
}
@article{jaleel_memory_2010,
	title = {Memory characterization of workloads using instrumentation-driven simulation},
	journal = {Web Copy: http://www. glue. umd. edu/ajaleel/workload},
	author = {Jaleel, Aamer},
	year = {2010},
	file = {SPECanalysis.pdf:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/2VAVTTVX/SPECanalysis.pdf:application/pdf}
}
@inproceedings{kannan_optimizing_2013,
	title = {Optimizing {Checkpoints} {Using} {NVM} as {Virtual} {Memory}},
	doi = {10.1109/IPDPS.2013.69},
	abstract = {Rapid checkpointing will remain key functionality for next generation high end machines. This paper explores the use of node-local nonvolatile memories (NVM) such as phase-change memory, to provide frequent, low overhead checkpoints. By adapting existing multi-level checkpoint techniques, we devise new methods, termed NVM-checkpoints, that efficiently store checkpoints on both local and remote node NVM. The checkpoint frequencies are guided by failure models that capture the expected accessibility of such data after failure. To lower overheads, NVM-checkpoints reduce the NVM and interconnect bandwidth used with a novel pre-copy mechanism, which incrementally moves checkpoint data from DRAM to NVM before a local checkpoint is started. This reduces local checkpoint cost by limiting the instantaneous data volume moved at checkpoint time, thereby freeing bandwidth for use by applications. In fact, the pre-copy method can reduce peak interconnect usage up to 46\%. Since our approach treats NVM as memory rather than as 'Ramdisk', pre-copying can be generalized to directly move data to remote NVMs. This results in 40\% faster application execution times compared to asynchronous approaches not using pre-copying.},
	booktitle = {2013 {IEEE} 27th {International} {Symposium} on {Parallel} {Distributed} {Processing}},
	author = {Kannan, S. and Gavrilovska, A. and Schwan, K. and Milojicic, D.},
	month = may,
	year = {2013},
	keywords = {Bandwidth, CCF-B, checkpoint frequency, checkpointing, checkpoint optimization, data accessibility, DMA, DRAM, DRAM chips, failure analysis, failure model, fault tolerance, Hardware, Helpful, information retrieval, local node NVM, Memory bandwidth, multilevel checkpoint technique, next generation high end machine, nonvolatile memory, Non volatile memory (NVM), optimisation, PCM, Peer-to-peer computing, Perusal, Phase change materials, Pre-Copy, precopy mechanism, Random access memory, remote node NVM, Top, virtual machines, virtual memory, virtual storage},
	pages = {29--40},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/EZNFCJ6Z/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/F4GB4TSE/Kannan 等. - 2013 - Optimizing Checkpoints Using NVM as Virtual Memory.pdf:application/pdf}
}
@inproceedings{khan_improving_2014,
	title = {Improving cache performance using read-write partitioning},
	doi = {10.1109/HPCA.2014.6835954},
	abstract = {Cache read misses stall the processor if there are no independent instructions to execute. In contrast, most cache write misses are off the critical path of execution, since writes can be buffered in the cache or the store buffer. With few exceptions, cache lines that serve loads are more critical for performance than cache lines that serve only stores. Unfortunately, traditional cache management mechanisms do not take into account this disparity between read-write criticality. This paper proposes a Read-Write Partitioning (RWP) policy that minimizes read misses by dynamically partitioning the cache into clean and dirty partitions, where partitions grow in size if they are more likely to receive future read requests. We show that exploiting the differences in read-write criticality provides better performance over prior cache management mechanisms. For a single-core system, RWP provides 5\% average speedup across the entire SPEC CPU2006 suite, and 14\% average speedup for cache-sensitive benchmarks, over the baseline LRU replacement policy. We also show that RWP can perform within 3\% of a new yet complex instruction-address-based technique, Read Reference Predictor (RRP), that bypasses cache lines which are unlikely to receive any read requests, while requiring only 5.4\% of RRP's state overhead. On a 4-core system, our RWP mechanism improves system throughput by 6\% over the baseline and outperforms three other state-of-the-art mechanisms we evaluate.},
	booktitle = {2014 {IEEE} 20th {International} {Symposium} on {High} {Performance} {Computer} {Architecture}},
	author = {Khan, S. and Alameldeen, A.R. and Wilkerson, C. and Mutluy, O. and Jimenezz, D.A.},
	month = feb,
	year = {2014},
	keywords = {benchmark testing, buffer storage, cache management, cache performance, cache storage, critical path, critical path analysis, Educational institutions, Memory management, Prediction algorithms, read reference predictor, read-write partitioning, Resource management, RRP, single-core system, store buffer, Throughput},
	pages = {452--463},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/2UHD7QB3/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/C6BDN4F8/Khan et al. - 2014 - Improving cache performance using read-write parti.pdf:application/pdf}
}
@inproceedings{lee_architecting_2009,
	title = {Architecting {Phase} {Change} {Memory} {As} a {Scalable} {Dram} {Alternative}},
	isbn = {978-1-60558-526-0},
	doi = {10.1145/1555754.1555758},
	abstract = {Memory scaling is in jeopardy as charge storage and sensing mechanisms become less reliable for prevalent memory technologies, such as DRAM. In contrast, phase change memory (PCM) storage relies on scalable current and thermal mechanisms. To exploit PCM's scalability as a DRAM alternative, PCM must be architected to address relatively long latencies, high energy writes, and finite endurance. We propose, crafted from a fundamental understanding of PCM technology parameters, area-neutral architectural enhancements that address these limitations and make PCM competitive with DRAM. A baseline PCM system is 1.6x slower and requires 2.2x more energy than a DRAM system. Buffer reorganizations reduce this delay and energy gap to 1.2x and 1.0x, using narrow rows to mitigate write energy and multiple rows to improve locality and write coalescing. Partial writes enhance memory endurance, providing 5.6 years of lifetime. Process scaling will further reduce PCM energy costs and improve endurance.},
	urldate = {2014-05-26},
	booktitle = {Proceedings of the 36th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	author = {Lee, Benjamin C. and Ipek, Engin and Mutlu, Onur and Burger, Doug},
	year = {2009},
	keywords = {CCF-A, dram alternative, endurance, energy, PCM, performance, phase change memory, power, Scalability},
	pages = {2--13},
	file = {ACM - Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/VG7ZBJR7/Lee - Architecting Phase Change Memory As a Scalable Dram Alternative.pdf:application/pdf}
}

@inproceedings{park_power_2011,
	title = {Power management of hybrid {DRAM}/{PRAM}-based main memory},
	abstract = {Hybrid main memory consisting of DRAM and non-volatile memory is attractive since the non-volatile memory can give the advantage of low standby power while DRAM provides high performance and better active power. In this work, we address the power management of such a hybrid main memory consisting of DRAM and phase-change RAM (PRAM). In order to reduce DRAM refresh energy which occupies a significant portion of total memory energy, we present a runtime-adaptive method of DRAM decay. In addition, we present two methods, DRAM bypass and dirty data keeping, for further reduction in refresh energy and memory access latency, respectively. The experiments show that by reducing DRAM refreshes, we can obtain 23.5\% 94.7\% reduction in the energy consumption with negligible performance overhead compared with the conventional DRAM-only main memory.},
	booktitle = {2011 48th {ACM}/{EDAC}/{IEEE} {Design} {Automation} {Conference}},
	author = {Park, Hyunsun and Yoo, Sungjoo and Lee, Sunggu},
	month = jun,
	year = {2011},
	keywords = {CCF-B, computer power supplies, concurrency theory, dirty data keeping, DRAM, DRAM bypass, DRAM chips, DRAM decay, DRAM refresh energy reduction, DRAM refreshes reduction, energy consumption, hybrid DRAM-based main memory, Hybrid power systems, hybrid PRAM-based main memory, Memory management, nonvolatile memory, phase-change RAM, Phase change random access memory, power, power management, Radiation detectors, random-access storage, refresh, runtime, runtime-adaptive method, Skim, Top, total memory energy},
	pages = {59--64},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/D3HDMHAD/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/3G2Z89J3/Park 等. - 2011 - Power management of hybrid DRAMPRAM-based main me.pdf:application/pdf}
}
@inproceedings{patel_marss_2011,
	title = {{MARSS}: {A} full system simulator for multicore x86 {CPUs}},
	shorttitle = {{MARSS}},
	abstract = {We present MARSS, an open source, fast, full system simulation tool built on QEMU to support cycle-accurate simulation of superscalar homogeneous and heterogeneous multicore x86 processors. MARSS includes detailed models of coherent caches, interconnections, chipsets, memory and IO devices. MARSS simulates the execution of all software components in the system, including unmodified binaries of applications, OS and libraries.},
	booktitle = {2011 48th {ACM}/{EDAC}/{IEEE} {Design} {Automation} {Conference}},
	author = {Patel, A. and Afram, F. and Chen, Shunfei and Ghose, K.},
	month = jun,
	year = {2011},
	keywords = {benchmark testing, Context modeling, Emulation, Emulator, full system simulator, Full-System Simulator, Heterogeneous Multi-core Systems, Instruction sets, Kernel, MARSS, multicore processing, multicore x86 CPU processor, Multi-core x86 CPU Simulator, multiprocessing systems, QEMU, Random access memory, switches},
	pages = {1050--1055},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/AIMA3S3P/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/PNEZ4RMA/Patel et al. - 2011 - MARSS A full system simulator for multicore x86 C.pdf:application/pdf}
}

@inproceedings{petrini_scaling_2002,
	title = {Scaling to {Thousands} of {Processors} with {Buffered} {Coscheduling}},
	abstract = {In this paper we describe Buffered Coscheduling, a new approach to design  the system software of large scale parallel computers. A buffered coscheduled  system can tolerate inefficient programs, programs that have communication and  load imbalance, and can substantially increase the resource utilization, by overlapping  computation, communication and I/O over several jobs. In addition to that,  Buffered Coscheduling creates a framework to easily implement fault-tolerance,  arguably the most important problem to solve to build usable, large scale parallel  computers.},
	booktitle = {{IN} {SCALING} {TO} {NEW} {HEIGHTS} {WORKSHOP}},
	author = {Petrini, Fabrizio},
	year = {2002},
	file = {Citeseer - Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/IJIBIAS7/Petrini - 2002 - Scaling to Thousands of Processors with Buffered C.pdf:application/pdf;Citeseer - Snapshot:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/K7ZNI6K2/summary.html:text/html}
}
@article{poremba_nvmain_2015,
	title = {{NVMain} 2.0: {Architectural} {Simulator} to {Model} ({Non}-){Volatile} {Memory} {Systems}},
	volume = {PP},
	issn = {1556-6056},
	shorttitle = {{NVMain} 2.0},
	doi = {10.1109/LCA.2015.2402435},
	abstract = {In this letter, a flexible memory simulator – NVMain 2.0, is introduced to help the community for modeling not only commodity DRAMs but also emerging memory technologies, such as die-stacked DRAM caches, non-volatile memories (e.g., STT-RAM, PCRAM, and ReRAM) including multi-level cells (MLC), and hybrid non-volatile plus DRAM memory systems. Compared to existing memory simulators, NVMain 2.0 features a flexible user interface with compelling simulation speed and the capability of providing subarray- level parallelism, fine-grained refresh, MLC and data encoder modeling, and distributed energy profiling.},
	number = {99},
	journal = {Computer Architecture Letters},
	author = {Poremba, M. and Zhang, T. and Xie, Y.},
	year = {2015},
	keywords = {Accuracy, Computational modeling, Computer architecture, nonvolatile memory, Phase change random access memory, Skim, Timing},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/X6S6F6Q9/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/RHDPRATU/Poremba et al. - 2015 - NVMain 2.0 Architectural Simulator to Model (Non-.pdf:application/pdf}
}
@inproceedings{prvulovic_revive_2002,
	title = {{ReVive}: cost-effective architectural support for rollback recovery in shared-memory multiprocessors},
	shorttitle = {{ReVive}},
	doi = {10.1109/ISCA.2002.1003567},
	abstract = {This paper presents ReVive, a novel general-purpose rollback recovery mechanism for shared-memory multiprocessors. ReVive carefully balances the conflicting requirements of availability, performance, and hardware cost. ReVive performs checkpointing, logging, and distributed parity protection, all memory-based. It enables recovery from a wide class of errors, including the permanent loss of an entire node. To maintain high performance, ReVive includes specialized hardware that performs frequent operations in the background, such as log and parity updates. To keep the cost low, more complex checkpointing and recovery functions are performed in software, while the hardware modifications are limited to the directory controllers of the machine. Our simulation results on a 16-processor system indicate that the average error-free execution time overhead of using ReVive is only 6.3\%, while the achieved availability is better than 99.999\% even when the errors occur as often as once per day},
	booktitle = {29th {Annual} {International} {Symposium} on {Computer} {Architecture}, 2002. {Proceedings}},
	author = {Prvulovic, M. and Zhang, Zheng and Torrellas, J.},
	year = {2002},
	keywords = {architectural support, Availability, Bit error rate, CCF-A, checkpointing, distributed parity protection, error correction, error detection, Hardware, hardware cost, integrated circuit reliability, performance, Performance evaluation, Performance loss, Power system reliability, Protection, Redundancy, ReVive, rollback recovery, shared-memory multiprocessors, shared memory systems, simulation results, Skim, system recovery, Top},
	pages = {111--122},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/8Q4ASNN9/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/6VQMFC6B/Prvulovic 等. - 2002 - ReVive cost-effective architectural support for r.pdf:application/pdf}
}

@inproceedings{qureshi_morphable_2010,
	title = {Morphable {Memory} {System}: {A} {Robust} {Architecture} for {Exploiting} {Multi}-level {Phase} {Change} {Memories}},
	isbn = {978-1-4503-0053-7},
	shorttitle = {Morphable {Memory} {System}},
	doi = {10.1145/1815961.1815981},
	abstract = {Phase Change Memory (PCM) is emerging as a scalable and power efficient technology to architect future main memory systems. The scalability of PCM is enhanced by the property that PCM devices can store multiple bits per cell. While such Multi-Level Cell (MLC) devices can offer high density, this benefit comes at the expense of increased read latency, which can cause significant performance degradation. This paper proposes Morphable Memory System (MMS), a robust architecture for efficiently incorporating MLC PCM devices in main memory. MMS is based on observation that memory requirement varies between workloads, and systems are typically over-provisioned in terms of memory capacity. So, during a phase of low memory usage, some of the MLC devices can be operated at fewer bits per cell to obtain lower latency. When the workload requires full memory capacity, these devices can be restored to high density MLC operation to have full main-memory capacity. We provide the runtime monitors, the hardware-OS interface, and the detailed mechanism for implementing MMS. Our evaluations on an 8-core 8GB MLC PCM-based system show that MMS provides, on average, low latency access for 95\% of all memory requests, thereby improving overall system performance by 40\%.},
	urldate = {2016-03-03},
	booktitle = {Proceedings of the 37th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	author = {Qureshi, Moinuddin K. and Franceschini, Michele M. and Lastras-Montaño, Luis A. and Karidis, John P.},
	year = {2010},
	keywords = {morphable memory, multi-level cell, phase change memory},
	pages = {153--162},
	file = {ACM Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/A8EBTHME/Qureshi et al. - 2010 - Morphable Memory System A Robust Architecture for.pdf:application/pdf}
}
@inproceedings{qureshi_scalable_2009,
	title = {Scalable {High} {Performance} {Main} {Memory} {System} {Using} {Phase}-change {Memory} {Technology}},
	isbn = {978-1-60558-526-0},
	doi = {10.1145/1555754.1555760},
	abstract = {The memory subsystem accounts for a significant cost and power budget of a computer system. Current DRAM-based main memory systems are starting to hit the power and cost limit. An alternative memory technology that uses resistance contrast in phase-change materials is being actively investigated in the circuits community. Phase Change Memory (PCM) devices offer more density relative to DRAM, and can help increase main memory capacity of future systems while remaining within the cost and power constraints. In this paper, we analyze a PCM-based hybrid main memory system using an architecture level model of PCM.We explore the trade-offs for a main memory system consisting of PCMstorage coupled with a small DRAM buffer. Such an architecture has the latency benefits of DRAM and the capacity benefits of PCM. Our evaluations for a baseline system of 16-cores with 8GB DRAM show that, on average, PCM can reduce page faults by 5X and provide a speedup of 3X. As PCM is projected to have limited write endurance, we also propose simple organizational and management solutions of the hybrid memory that reduces the write traffic to PCM, boosting its lifetime from 3 years to 9.7 years.},
	urldate = {2014-07-04},
	booktitle = {Proceedings of the 36th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	author = {Qureshi, Moinuddin K. and Srinivasan, Vijayalakshmi and Rivers, Jude A.},
	year = {2009},
	keywords = {CCF-A, dram caching, Helpful, phase change memory, Skim, Top, wear leveling},
	pages = {24--33},
	file = {ACM Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/PJEE4W8Z/Scalable High Performance Main Memory System Using Phase-Change Memory Technology.pdf:application/pdf}
}
@inproceedings{rajachandrasekar_1_2013,
	title = {A 1 {PB}/s {File} {System} to {Checkpoint} {Three} {Million} {MPI} {Tasks}},
	isbn = {978-1-4503-1910-2},
	doi = {10.1145/2462902.2462908},
	abstract = {With the massive scale of high-performance computing systems, long-running scientific parallel applications periodically save the state of their execution to files called checkpoints to recover from system failures. Checkpoints are stored on external parallel file systems, but limited bandwidth makes this a time-consuming operation. Multilevel checkpointing systems, like the Scalable Checkpoint/Restart (SCR) library, alleviate this bottleneck by caching checkpoints in storage located close to the compute nodes. However, most large scale systems do not provide file storage on compute nodes, preventing the use of SCR. We have implemented a novel user-space file system that stores data in main memory and transparently spills over to other storage, like local flash memory or the parallel file system, as needed. This technique extends the reach of libraries like SCR to systems where they otherwise could not be used. Furthermore, we expose file contents for Remote Direct Memory Access, allowing external tools to copy checkpoints to the parallel file system in the background with reduced CPU interruption. Our file system scales linearly with node count and delivers a 1{\textasciitilde}PB/s throughput at three million MPI processes, which is 20x faster than the system RAM disk and 1000x faster than the parallel file system.},
	urldate = {2014-03-25},
	booktitle = {Proceedings of the 22Nd {International} {Symposium} on {High}-performance {Parallel} and {Distributed} {Computing}},
	author = {Rajachandrasekar, Raghunath and Moody, Adam and Mohror, Kathryn and Panda, Dhabaleswar K. (DK)},
	year = {2013},
	keywords = {CCF-B, fault-tolerance, file systems, Helpful, HPC, multilevel checkpointing, persistent- memory, RDMA, Skim, SSD, Top},
	pages = {143--154},
	file = {ACM Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/TWSUWF3M/Rajachandrasekar 等. - 2013 - A 1 PBs File System to Checkpoint Three Million M.pdf:application/pdf}
}
@inproceedings{ramos_page_2011,
	title = {Page {Placement} in {Hybrid} {Memory} {Systems}},
	isbn = {978-1-4503-0102-2},
	doi = {10.1145/1995896.1995911},
	abstract = {Phase-Change Memory (PCM) technology has received substantial attention recently. Because PCM is byte-addressable and exhibits access times in the nanosecond range, it can be used in main memory designs. In fact, PCM has higher density and lower idle power consumption than DRAM. Unfortunately, PCM is also slower than DRAM and has limited endurance. For these reasons, researchers have proposed memory systems that combine a small amount of DRAM and a large amount of PCM. In this paper, we propose a new hybrid design that features a hardware-driven page placement policy. The policy relies on the memory controller (MC) to monitor access patterns, migrate pages between DRAM and PCM, and translate the memory addresses coming from the cores. Periodically, the operating system updates its page mappings based on the translation information used by the MC. Detailed simulations of 27 workloads show that our system is more robust and exhibits lower energy-delay2 than state-of-the-art hybrid systems.},
	urldate = {2014-08-25},
	booktitle = {Proceedings of the {International} {Conference} on {Supercomputing}},
	author = {Ramos, Luiz E. and Gorbatov, Eugene and Bianchini, Ricardo},
	year = {2011},
	keywords = {CCF-B, hybrid memory, memory controller, phase-change memory},
	pages = {85--95},
	file = {ACM Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/NWG5US5D/Ramos 等 - Page Placement in Hybrid Memory Systems.pdf:application/pdf}
}
@article{rosenfeld_dramsim2_2011,
	title = {{DRAMSim}2: {A} {Cycle} {Accurate} {Memory} {System} {Simulator}},
	volume = {10},
	issn = {1556-6056},
	shorttitle = {{DRAMSim}2},
	doi = {10.1109/L-CA.2011.4},
	abstract = {In this paper we present DRAMSim2, a cycle accurate memory system simulator. The goal of DRAMSim2 is to be an accurate and publicly available DDR2/3 memory system model which can be used in both full system and trace-based simulations. We describe the process of validating DRAMSim2 timing against manufacturer Verilog models in an effort to prove the accuracy of simulation results. We outline the combination of DRAMSim2 with a cycle-accurate x86 simulator that can be used to perform full system simulations. Finally, we discuss DRAMVis, a visualization tool that can be used to graph and compare the results of DRAMSim2 simulations.},
	number = {1},
	journal = {Computer Architecture Letters},
	author = {Rosenfeld, P. and Cooper-Balis, E. and Jacob, B.},
	month = jan,
	year = {2011},
	keywords = {Computational modeling, cycle accurate memory system simulator, DDR2/3 memory system model, DRAM, DRAM chips, DRAMSim2 simulation, DRAMSim2 timing, Driver circuits, Hardware design languages, Load modeling, memory architecture, memory cards, Object oriented modeling, Primary memory, Random access memory, simulation, Timing, trace-based simulation, Verilog model, visualization tool},
	pages = {16--19},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/5GGV59S2/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/G5VETQ2E/Rosenfeld et al. - 2011 - DRAMSim2 A Cycle Accurate Memory System Simulator.pdf:application/pdf}
}
@inproceedings{sorin_safetynet_2002,
	title = {{SafetyNet}: improving the availability of shared memory multiprocessors with global checkpoint/recovery},
	shorttitle = {{SafetyNet}},
	doi = {10.1109/ISCA.2002.1003568},
	abstract = {We develop an availability solution, called SafetyNet, that uses a unified, lightweight checkpoint/recovery mechanism to support multiple long-latency fault detection schemes. At an abstract level, SafetyNet logically maintains multiple, globally consistent checkpoints of the state of a shared memory multiprocessor, and it recovers to a pre-fault checkpoint of the system and re-executes if a fault is detected. SafetyNet efficiently coordinates checkpoints across the system in logical time and uses "logically atomic" coherence transactions to free checkpoints of transient coherence state. SafetyNet minimizes performance overhead by pipelining checkpoint validation with subsequent parallel execution. We illustrate SafetyNet avoiding system crashes due to either dropped coherence messages or the loss of an interconnection network switch. Using a full-system simulation of a 16-way multiprocessor running commercial workloads, we find that SafetyNet: 1) adds statistically insignificant runtime overhead in the common-case of fault-free execution, and 2) avoids a crash when tolerated faults occur},
	booktitle = {29th {Annual} {International} {Symposium} on {Computer} {Architecture}, 2002. {Proceedings}},
	author = {Sorin, D.J. and Martin, M.M.K. and Hill, M.D. and Wood, D.A},
	year = {2002},
	keywords = {Ash, Availability, buffered messages, CCF-A, checkpoint recovery mechanism, Circuit faults, Crosstalk, Fault detection, fault tolerant computing, Frequency, Hardware, long-latency fault detection, multiprocessor, Multiprocessor interconnection networks, network servers, parallel architectures, parallel processing, pipeline processing, SafetyNet, shared memory multiprocessor, shared memory systems, Skim, switches, system recovery, Top, transient coherence state},
	pages = {123--134},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/77UW5V8Z/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/W6TUCA45/Sorin 等. - 2002 - SafetyNet improving the availability of shared me.pdf:application/pdf}
}

@article{spec2006benchmark,
  title={{SPEC} {CPU}2006},
  journal={http://www.spec.org/cpu2006},
}

@techreport{stearley_rmpi_2011,
	title = {Rmpi: {Increasing} {Fault} {Resiliency} in a {Message}-{Passing} {Environment}.},
	shorttitle = {Rmpi},
	language = {English},
	number = {SAND2011-2488},
	urldate = {2015-11-15},
	institution = {Sandia National Laboratories},
	author = {Stearley, Jon R. and Laros, James H. and Ferreira, Kurt Brian and Pedretti, Kevin Thomas Tauke and Oldfield, Ron A. and Riesen, Rolf (IBM Research and Brightwell, Ronald Brian},
	month = apr,
	year = {2011},
	keywords = {and information science, computers, computing, design, general and miscellaneous//mathematics, reliability, replicas, tolerance}
}

@article{stevens_integrated_2013,
	title = {An {Integrated} {Simulation} {Infrastructure} for the {Entire} {Memory} {Hierarchy}: {Cache}, {DRAM}, {Nonvolatile} {Memory}, and {Disk}},
	volume = {17},
	number = {1},
	journal = {Intel Technology Journal},
	author = {Stevens, Jim and Tschirhart, Paul and Chang, Mu-Tien and Bhati, Ishwar and Enns, Peter and Greensky, James and Chisti, Zeshan and Lu, SL and Jacob, B},
	year = {2013},
	keywords = {Skim},
	pages = {184--200},
	file = {itj2013.pdf:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/I7GDXW2R/itj2013.pdf:application/pdf}
}
@inproceedings{stuecheli_virtual_2010,
	title = {The {Virtual} {Write} {Queue}: {Coordinating} {DRAM} and {Last}-level {Cache} {Policies}},
	isbn = {978-1-4503-0053-7},
	shorttitle = {The {Virtual} {Write} {Queue}},
	doi = {10.1145/1815961.1815972},
	abstract = {In computer architecture, caches have primarily been viewed as a means to hide memory latency from the CPU. Cache policies have focused on anticipating the CPU's data needs, and are mostly oblivious to the main memory. In this paper, we demonstrate that the era of many-core architectures has created new main memory bottlenecks, and mandates a new approach: coordination of cache policy with main memory characteristics. Using the cache for memory optimization purposes, we propose a Virtual Write Queue which dramatically expands the memory controller's visibility of processor behavior, at low implementation overhead. Through memory-centric modification of existing policies, such as scheduled writebacks, this paper demonstrates that performance limiting effects of highly-threaded architectures can be overcome. We show that through awareness of the physical main memory layout and by focusing on writes, both read and write average latency can be shortened, memory power reduced, and overall system performance improved. Through full-system cycle-accurate simulations of SPEC cpu2006, we demonstrate that the proposed Virtual Write Queue achieves an average 10.9\% system-level throughput improvement on memory-intensive workloads, along with an overall reduction of 8.7\% in memory power across the whole suite.},
	urldate = {2016-03-18},
	booktitle = {Proceedings of the 37th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	author = {Stuecheli, Jeffrey and Kaseridis, Dimitris and Daly, David and Hunter, Hillery C. and John, Lizy K.},
	year = {2010},
	keywords = {cache-replacement, cmp many-core, ddr ddr2 ddr3, DRAM, dram-parameters, last-level-cache, memory-scheduling writeback, page-mode, write-queue, write-scheduling},
	pages = {72--82},
	file = {ACM Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/SCEIVAUI/Stuecheli et al. - 2010 - The Virtual Write Queue Coordinating DRAM and Las.pdf:application/pdf}
}
@inproceedings{wu_car_2012,
	title = {{CAR}: {Securing} {PCM} {Main} {Memory} {System} with {Cache} {Address} {Remapping}},
	volume = {0},
	shorttitle = {{CAR}},
	doi = {10.1109/ICPADS.2012.90},
	abstract = {Phase Change Memory (PCM) has emerged as a promising alternative of DRAM to provide energy-efficient and high-capacity memory for high performance servers. A new DRAM + PCM hybrid memory architecture has been proposed to leverage PCM's high density and DRAM's robustness and performance. One of the big challenges of PCM is its limited write endurance (10{\textasciicircum}7 {\textasciitilde} 10{\textasciicircum}8 times per cell). By knowing the association between DRAM and PCM, malicious software can easily force DRAM cache to be flushed continuously, which produces writes to certain PCM cells repeatedly (known as selective attack) and wears out PCM. Although existing wear-leveling approaches could evenly distribute writes under selective attack, the overall endurance of PCM is still severely impacted, and therefore it is suboptimal. In this paper, we propose Cache Address Remapping (CAR), that can adaptively remap DRAM cache address, to hide the association between DRAM and PCM. Moreover, CAR can minimize the write-back traffic to PCM under selective attack by uniformly distributing the writes to a single cache set into different cache sets. We propose a practical and low overhead implementation of CAR, called RanCAR. Experimental results show that CAR could reduce DRAM cache miss rate by {\textasciitilde}4600x under selective attack, and prolong PCM lifetime from several minutes to 13.8 years on average.},
	booktitle = {2013 {International} {Conference} on {Parallel} and {Distributed} {Systems}},
	author = {Wu, Gang and Zhang, Huxing and Dong, Yaozu and Hu, Jingtong},
	year = {2012},
	keywords = {Address Remapping, CCF-C, endurance, hybrid memory system, Malicious Attack, phase change memory, Set Association相关？, Skim, Top},
	pages = {628--635},
	file = {IEEE Computer Snapshot:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/IKGXKQGC/4903a628-abs.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/UPSK2QCP/Wu - CAR Securing PCM Main Memory System with Cache Address Remapping.pdf:application/pdf}
}

@inproceedings{yoon_row_2012,
	title = {Row buffer locality aware caching policies for hybrid memories},
	doi = {10.1109/ICCD.2012.6378661},
	abstract = {Phase change memory (PCM) is a promising technology that can offer higher capacity than DRAM. Unfortunately, PCM's access latency and energy are higher than DRAM's and its endurance is lower. Many DRAM-PCM hybrid memory systems use DRAM as a cache to PCM, to achieve the low access latency and energy, and high endurance of DRAM, while taking advantage of PCM's large capacity. A key question is what data to cache in DRAM to best exploit the advantages of each technology while avoiding its disadvantages as much as possible. We propose a new caching policy that improves hybrid memory performance and energy efficiency. Our observation is that both DRAM and PCM banks employ row buffers that act as a cache for the most recently accessed memory row. Accesses that are row buffer hits incur similar latencies (and energy consumption) in DRAM and PCM, whereas accesses that are row buffer misses incur longer latencies (and higher energy consumption) in PCM. To exploit this, we devise a policy that avoids accessing in PCM data that frequently causes row buffer misses because such accesses are costly in terms of both latency and energy. Our policy tracks the row buffer miss counts of recently used rows in PCM, and caches in DRAM the rows that are predicted to incur frequent row buffer misses. Our proposed caching policy also takes into account the high write latencies of PCM, in addition to row buffer locality. Compared to a conventional DRAM-PCM hybrid memory system, our row buffer locality-aware caching policy improves system performance by 14\% and energy efficiency by 10\% on data-intensive server and cloud-type workloads. The proposed policy achieves 31\% performance gain over an all-PCM memory system, and comes within 29\% of the performance of an allDRAM memory system (not taking PCM's capacity benefit into account) on evaluated workloads.},
	booktitle = {2012 {IEEE} 30th {International} {Conference} on {Computer} {Design}},
	author = {Yoon, HanBin and Meza, J. and Ausavarungnirun, R. and Harding, R.A. and Mutlu, O.},
	month = sep,
	year = {2012},
	keywords = {Arrays, cloud-type workloads, data-intensive server, DRAM cache, DRAM chips, DRAM-PCM hybrid memory systems, efficiency 10 percent, Energy efficiency, Heuristic algorithms, low access energy, low access latency, Memory management, Microprocessors, Phase change materials, phase change memories, phase change memory, Random access memory, row buffer locality aware caching policies},
	pages = {337--344},
	file = {IEEE Xplore Abstract Record:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/CD9RXESZ/abs_all.html:text/html;IEEE Xplore Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/JN6A2NZP/rowbuffer-aware-caching_iccd12.pdf:application/pdf}
}
@inproceedings{zhou_leveraging_2013,
	title = {Leveraging {Phase} {Change} {Memory} to {Achieve} {Efficient} {Virtual} {Machine} {Execution}},
	isbn = {978-1-4503-1266-0},
	doi = {10.1145/2451512.2451547},
	abstract = {Virtualization technology is being widely adopted by servers and data centers in the cloud computing era to improve resource utilization and energy efficiency. Nevertheless, the heterogeneous memory demands from multiple virtual machines (VM) make it more challenging to design efficient memory systems. Even worse, mission critical VM management activities (e.g. checkpointing) could incur significant runtime overhead due to intensive IO operations. In this paper, we propose to leverage the adaptable and non-volatile features of the emerging phase change memory (PCM) to achieve efficient virtual machine execution. Towards this end, we exploit VM-aware PCM management mechanisms, which 1) smartly tune SLC/MLC page allocation within a single VM and across different VMs and 2) keep critical checkpointing pages in PCM to reduce I/O traffic. Experimental results show that our single VM design (IntraVM) improves performance by 10\% and 20\% compared to pure SLC- and MLC- based systems. Further incorporating VM-aware resource management schemes (IntraVM+InterVM) increases system performance by 15\%. In addition, our design saves 46\% of checkpoint/restore duration and reduces 50\% of overall IO penalty to the system.},
	urldate = {2014-07-22},
	booktitle = {Proceedings of the 9th {ACM} {SIGPLAN}/{SIGOPS} {International} {Conference} on {Virtual} {Execution} {Environments}},
	author = {Zhou, Ruijin and Li, Tao},
	year = {2013},
	keywords = {CCF-B, checkpointing, Memory management, phase change memory, Skim, Top, virtualization},
	pages = {179--190},
	file = {ACM Full Text PDF:/home/oryxfea/.zotero/zotero/6f7ucgr9.default/zotero/storage/R5VG6IQH/zhou 和 Li 等 - Leveraging Phase Change Memory to Achieve Efficient Virtual Machine Execution.pdf:application/pdf}
}